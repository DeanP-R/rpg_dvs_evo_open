<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=9" />
    <meta name="generator" content="Doxygen 1.8.13" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>EVO: Event based Visual Odometry: EVO: Event based Visual Odometry</title>
    <link href="tabs.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="dynsections.js"></script>
    <link href="search/search.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="search/searchdata.js"></script>
    <script type="text/javascript" src="search/search.js"></script>
    <link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>

<body>
    <div id="top">
        <!-- do not remove this div, it is closed by doxygen! -->
        <div id="titlearea">
            <table cellspacing="0" cellpadding="0">
                <tbody>
                    <tr style="height: 56px;">
                        <td id="projectalign" style="padding-left: 0.5em;">
                            <div id="projectname">EVO: Event based Visual Odometry
                            </div>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <!-- end header part -->
        <!-- Generated by Doxygen 1.8.13 -->
        <script type="text/javascript">
            var searchBox = new SearchBox("searchBox", "search", false, 'Search');
        </script>
        <script type="text/javascript" src="menudata.js"></script>
        <script type="text/javascript" src="menu.js"></script>
        <script type="text/javascript">
            $(function () {
                initMenu('', true, false, 'search.php', 'Search');
                $(document).ready(function () { init_search(); });
            });
        </script>
        <div id="main-nav"></div>
        <!-- window showing the filter options -->
        <div id="MSearchSelectWindow" onmouseover="return searchBox.OnSearchSelectShow()"
            onmouseout="return searchBox.OnSearchSelectHide()" onkeydown="return searchBox.OnSearchSelectKey(event)">
        </div>

        <!-- iframe showing the search results (closed by default) -->
        <div id="MSearchResultsWindow">
            <iframe src="javascript:void(0)" frameborder="0" name="MSearchResults" id="MSearchResults">
            </iframe>
        </div>

    </div><!-- top -->
    <div class="header">
        <div class="headertitle">
            <div class="title">EVO: Event based Visual Odometry </div>
        </div>
    </div>
    <!--header-->
    <div class="contents">
        <div class="textblock">
            <p><a href="https://www.youtube.com/watch?v=bYqD2qZJlxE"></a></p>
            <p><a class="anchor" id="license"></a> </p>
            <h2>License, patent and credits</h2>
            <h3>Citation</h3>
            <p>This code implements the event-based visual odometry pipeline described in the paper <a
                    href="http://rpg.ifi.uzh.ch/docs/RAL16_EVO.pdf">EVO: A Geometric Approach to Event-Based 6-DOF
                    Parallel Tracking and Mapping in Real-time</a> by <a
                    href="https://supitalp.github.io/research/">Henri Rebecq</a>, Timo Horstschaefer, <a
                    href="https://sites.google.com/view/guillermogallego">Guillermo Gallego</a> and <a
                    href="http://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>.</p>
            <p>If you use any of this code, please cite the following publications:</p>
            <div class="fragment">
                <div class="line">@Article{RebecqEVO,</div>
                <div class="line"> author = {Rebecq, Henri and Horstschaefer, Timo and Gallego, Guillermo and
                    Scaramuzza, Davide},</div>
                <div class="line"> journal = {IEEE Robotics and Automation Letters}, </div>
                <div class="line"> title = {EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping
                    in Real Time}, </div>
                <div class="line"> year = {2017},</div>
                <div class="line"> volume = {2},</div>
                <div class="line"> number = {2},</div>
                <div class="line"> pages = {593-600},</div>
                <div class="line"> doi = {10.1109/LRA.2016.2645143}</div>
                <div class="line">}</div>
            </div><!-- fragment -->
            <div class="fragment">
                <div class="line">@InProceedings{Gehrig_2020_CVPR,</div>
                <div class="line"> author = {Daniel Gehrig and Mathias Gehrig and Javier Hidalgo-Carri\&#39;o and Davide
                    Scaramuzza},</div>
                <div class="line"> title = {Video to Events: Recycling Video Datasets for Event Cameras},</div>
                <div class="line"> booktitle = {{IEEE} Conf. Comput. Vis. Pattern Recog. (CVPR)},</div>
                <div class="line"> month = {June},</div>
                <div class="line"> year = {2020}</div>
                <div class="line">}</div>
            </div><!-- fragment -->
            <p>If you don't have an event camera, you can generate an artificial dataset using the above work.</p>
            <h3>Patent &amp; License</h3>
            <ul>
                <li>The proposed EVO method is <a
                        href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2018037079">patented</a>.
                    <pre class="fragment">  H. Rebecq, G. Gallego, D. Scaramuzza
  Simultaneous Localization and Mapping with an Event Camera
  Pub. No.: WO/2018/037079.  International Application No.: PCT/EP2017/071331
</pre>
                </li>
                <li>The license is [available here](LICENSE).</li>
            </ul>
            <h3>Acknowledgements</h3>
            <p>The open sourcing was curated by <a href="https://antonioterpin.github.io">Antonio Terpin</a> and <a
                    href="https://danielgehrig18.github.io">Daniel Gehrig</a>.</p>
            <h4>Table of contents</h4>
            <ol type="1">
                <li><a href="#getstarted">Getting started</a></li>
                <li><a href="#examples">Examples</a></li>
                <li><a href="#live">Running live</a></li>
                <li><a href="#todo">Further improvements</a></li>
                <li><a href="#ref">Additional resources on Event Cameras</a></li>
            </ol>
            <p>Remark that this is <b>research</b> code, any fitness for a particular purpose is disclaimed.</p>
            <p><a class="anchor" id="getstarted"></a> </p>
            <h2>Getting started</h2>
            <p>This software depends on <a href="https://www.ros.org/">ROS</a>. Installation instructions can be found
                <a href="http://wiki.ros.org/melodic/Installation/Ubuntu">here</a>. We have tested this software on
                Ubuntu 18.04 and ROS Melodic.
            </p>
            <ol type="1">
                <li>Create and initialize a new catkin workspace if needed
                    <pre class="fragment"> mkdir -p ~/catkin_ws/src &amp;&amp; cd ~/catkin_ws/
 catkin config \
     --init --mkdirs --extend /opt/ros/melodic \
     --merge-devel --cmake-args \
     -DCMAKE_BUILD_TYPE=Release -DCFLAGS=-Wno-error
</pre>
                </li>
                <li>Clone this repository
                    <pre class="fragment"> cd src/ &amp;&amp; git clone git@github.com:uzh-rpg/rpg_dvs_evo_open.git
</pre>
                </li>
                <li>
                    <p class="startli">Clone (and fix) dependencies </p>
                    <pre class="fragment"> ./rpg_dvs_evo_open/install.sh [ros-version] # [ros-version]: melodic, ..
</pre>
                    <p class="startli">The above commands do the following:</p>
                    <ul>
                        <li>First, we install the required packages. Namely,</li>
                    </ul>
                </li>
            </ol>
            <table class="doxtable">
                <tr>
                    <th>Package </th>
                    <th>Reason </th>
                </tr>
                <tr>
                    <td>ros-[ros-version]-sophus </td>
                    <td>Lie groups </td>
                </tr>
                <tr>
                    <td>ros-[ros-version]-pcl-ros </td>
                    <td>bridge between point clouds and 3D geometry processing in ros </td>
                </tr>
                <tr>
                    <td>ros-[ros-version]-eigen-conversions </td>
                    <td>Eigen data structures to ros geometry messages </td>
                </tr>
                <tr>
                    <td>libfftw3-dev, libfftw3-doc </td>
                    <td>fast fourier transform </td>
                </tr>
                <tr>
                    <td>libglew-dev </td>
                    <td>determining which OpenGL extensions are supported (svo) </td>
                </tr>
                <tr>
                    <td>libopencv-dev </td>
                    <td>opencv </td>
                </tr>
                <tr>
                    <td>libyaml-cpp-dev</td>
                    <td>yaml </td>
                </tr>
            </table>
            <ul>
                <li>Second, we clone the <a href="dependencies.yaml">repositories</a> evo relies on.</li>
            </ul>
            <p>Remark that with the above commands we install also the dependencies required to run the pipeline live.
                If you do not need them, you can comment the unnecessary packages from the dependencies.yaml file (<a
                    href="https://github.com/uzh-rpg/rpg_dvs_ros">davis driver</a>).</p>
            <ol type="1">
                <li>
                    <p class="startli">Build the packages</p>
                    <p class="startli">cd .. &amp;&amp; catkin build</p>
                </li>
            </ol>
            <p>Do not forget to source the workspace afterwards! </p>
            <pre class="fragment">source devel/setup.bash
</pre>
            <p><a class="anchor" id="examples"></a> </p>
            <h2>Examples</h2>
            <div class="image">
                <img src="images/cover_examples.png" alt="EVO: Event based Visual Odometry" />
            </div>
            <table class="doxtable">
                <tr>
                    <th align="left">Example </th>
                    <th align="left">Launch file </th>
                    <th align="left">Rosbag </th>
                </tr>
                <tr>
                    <td align="left">Multi-keyframe sequence </td>
                    <td align="left"><a href="dvs_tracking/launch/flyingroom.launch">flyingroom.launch</a> </td>
                    <td align="left"><a href="http://rpg.ifi.uzh.ch/data/EVO/code_examples/evo_flyingroom.bag">538
                            MB</a> </td>
                </tr>
                <tr>
                    <td align="left">Desk sequence </td>
                    <td align="left"><a href="dvs_tracking/launch/desk.launch">desk.launch</a> </td>
                    <td align="left"><a href="http://rpg.ifi.uzh.ch/data/EVO/code_examples/evo_desk.bag">145 MB</a>
                    </td>
                </tr>
            </table>
            <p>To download the rosbags, you can use the following command: </p>
            <pre class="fragment">cd /path/to/download/folder/
wget [rosbag link]
</pre>
            <p>For instance, the following will download the multi-keyframe sequence rosbag: </p>
            <pre class="fragment">wget http://rpg.ifi.uzh.ch/data/EVO/code_examples/evo_flyingroom.bag
</pre>
            <p>To run the pipeline from a rosbag, first start the pipeline as </p>
            <pre class="fragment">roslaunch dvs_tracking [launch-file] auto_trigger:=true
</pre>
            <p>where the specific launch file (fine-tuned) for each example is listed in the following table. The most
                interesting and repeatible one (due to the bootstrapping sequence, see <a href="#todo">further
                    improvements</a>) is the multi-keyframe sequence.</p>
            <p>Then, once the everything is loaded, run the rosbag as </p>
            <pre class="fragment">rosbag play [rosbag-file]
</pre>
            <p>For instance, </p>
            <pre class="fragment">rosbag play /path/to/download/folder/evo_flyingroom.bag
</pre>
            <p>Remark that we set the <code>auto_trigger</code> parameter to true. You can also set it to false and
                follow the instruction on <a href="#live">how to run it live</a>.</p>
            <p>If anything fails, just try it again (give it a couple of chances!), and make sure to follow exactly the
                above instructions.</p>
            <p>In the <a href="#todo">further improvements</a> section are outlined the things that might go wrong when
                running the code. To improve the reliability when playing the rosbags (running live is easier), consider
                using <code>-r .7</code>, to reduce the rate. This helps especially when the hardware is not powerful
                enough.</p>
            <p>For instance, </p>
            <pre class="fragment">rosbag play evo_flyingroom -r .7
</pre>
            <p>Eventually, setting <code>bootstrap_image_topic:=/dvs/image_raw</code> will bootstrap from traditional
                frames and later switch to only events. This is the most reliable way currently available to bootstrap.
            </p>
            <pre class="fragment">roslaunch dvs_tracking [launch-file] bootstrap_image_topic:=/dvs/image_raw auto_trigger:=true
</pre>
            <p><a class="anchor" id="live"></a> </p>
            <h2>Running live</h2>
            <p>To run the pipeline live, first adjust the template <a
                    href="dvs_tracking/launch/live.launch">live.launch</a> to your sensor and scene. You can follow the
                following steps. Further customization, such as which bootstrapper to use, are explained in the launch
                file itself.</p>
            <h3>Calibration</h3>
            <p>Make sure you have updated calibration files for your event camera, in the <code>rpg_calib</code> and
                that you have the calibration files for your DAVIS in <code>rpg_calib/dvs</code> and
                <code>rpg_calib/ncamera</code>.
            </p>
            <p>Make sure your <code>.yaml</code> files have the same format as the provided ones (<a
                    href="dvs_tracking/parameters/calib/DAVIS-evo.yaml">single camera format</a>, <a
                    href="dvs_tracking/parameters/calib/ncamera/DAVIS-evo.yaml">multiple cameras format</a>).</p>
            <p>See <a href="#ref">this section</a> for further references on calibration.</p>
            <h3>Tuning</h3>
            <p>Adjust the parameters in the launch file <a
                    href="dvs_tracking/launch/template.launch"><code>dvs_tracking/launch/template.launch</code></a>.</p>
            <p>Tuning is crucial for a good performance of the pipeline, in particular the <code>min_depth</code> and
                <code>max_depth</code> parameters in the mapping node, and the <code>bootstrap</code> node parameters.
            </p>
            <p>An explanation of all the parameters for each module of the pipeline can be found in the <a
                    href="https://github.com/antonioterpin/rpg_dvs_evo_open/wiki">Wiki</a>.</p>
            <p>The main parameters can be found in the template launch file, and are explained contextually. We still
                invite you to have a look at the Wiki, to discover further interesting features ;)</p>
            <table class="doxtable">
                <tr>
                    <th align="left">Module </th>
                </tr>
                <tr>
                    <td align="left"><a
                            href="https://github.com/antonioterpin/rpg_dvs_evo_open/wiki/Global-parameters">Global
                            parameters</a> </td>
                </tr>
                <tr>
                    <td align="left"><a
                            href="https://github.com/antonioterpin/rpg_dvs_evo_open/wiki/Bootstrapping">Bootstrapping</a>
                    </td>
                </tr>
                <tr>
                    <td align="left"><a
                            href="https://github.com/antonioterpin/rpg_dvs_evo_open/wiki/Mapping">Mapping</a>
                    </td>
                </tr>
                <tr>
                    <td align="left"><a
                            href="https://github.com/antonioterpin/rpg_dvs_evo_open/wiki/Tracking">Tracking</a>
                    </td>
                </tr>
            </table>
            <p>If you are not using the fronto-planar bootstrapper, then you might need to tune SVO.</p>
            <p>Remark that this might not be needed. You can test the svo tuning bootstrapping from traditional frames:
            </p>
            <pre class="fragment">roslaunch dvs_tracking live.launch bootstrap_image_topic:=[topic of raw frames] auto_trigger:=[true/false]
</pre>
            <p>For instance, <code>bootstrap_image_topic:=/dvs/image_raw</code>.</p>
            <h3>Run</h3>
            <p>The procedure is analogous to the one explained in the examples:</p>
            <ol type="1">
                <li>Run the ros core on the first terminal.
                    <pre class="fragment"> roscore
</pre>
                </li>
                <li>On a second terminal, launch the event camera driver.
                    <pre class="fragment"> rosrun davis_ros_driver davis_ros_driver
</pre>
                </li>
                <li>
                    <p class="startli">On another terminal, launch the pipeline, disabling the auto-triggering. </p>
                    <pre class="fragment"> roslaunch dvs_tracking live.launch auto_trigger:=false camera_name:=[calibration filename] events_topic:=[events topic]
</pre>
                    <p class="startli">If your calibrations filenames are <code>my_camera.yaml</code>, then use
                        <code>camera_name:=my_camera</code>. Make sure to use the same name for both calibration files.
                        If your sensor outputs events under the topic <code>/my_sensor/events</code>, then use
                        <code>events_topic:=/my_sensor/events</code>.
                    </p>
                    <p class="startli">For the SVO based bootstrapper, proceed with step 4. For the fronto-planar
                        bootstrapper, go to step 5.</p>
                    <p class="startli">If you want to bootstrap from traditional frames, you can use the command: </p>
                    <pre class="fragment">roslaunch dvs_tracking live.launch bootstrap_image_topic:=[topic raw frames] auto_trigger:=false
</pre>
                    <p class="startli">For instance <code>bootstrap_image_topic:=/dvs/image_raw</code>.</p>
                    <p class="startli">In this case, it makes sense to use the SVO-based bootstrapping only. This option
                        is recommended to debug/improve/extend the rest of the EVO pipeline, without worrying about the
                        quality of the bootstrapping.</p>
                </li>
                <li>You should see two rqt GUI. One is the SVO GUI. Reset and start the pipeline until it tracks
                    decently well.</li>
                <li>
                    <p class="startli">Press the <code>Bootstrap</code> button. This will automatically trigger the
                        pipeline.</p>
                    <p class="startli">Alternatively, it is also possible to trigger one module at the time:</p>
                    <ul>
                        <li>Press the <code>Start/Reset</code> button in <code>rqt_evo</code>. Perform a circle (or
                            more), and then press <code>Update</code>. This will trigger a map creation.</li>
                        <li>If the map looks correct, press <code>Switch to tracking</code> to start tracking with EVO.
                            If not, reiterate the map creation.</li>
                        <li>As the camera moves out of the current map, the latter will be automatically updated if the
                            <code>Map-Expansion</code> is enabled. You may disable <code>Map-Expansion</code> to track
                            high-speed motions using the current map (single keyframe tracking).
                        </li>
                    </ul>
                </li>
                <li>If anything fails, just press <code>Ctrl+C</code> and restart the live node ;)</li>
            </ol>
            <p>Some remarks:</p>
            <ul>
                <li>The calibration files paths will be built as
                    <code>$(find dvs_tracking)/parameters/calib/ncamera/$(arg camera_name).yaml</code> and
                    <code>$(find dvs_tracking)/parameters/calib/$(arg camera_name).yaml</code>, where
                    <code>camera_name</code> is specified as argument to the launch file. You can also set it as default
                    in <code>live.launch</code>.
                </li>
                <li>If your sensor provides frames under a topic <code>/my_sensor/image_raw</code>, and you want to
                    bootstrap from the traditional frames, you can use
                    <code>bootstrap_image_topic:=/my_sensor/image_raw</code>.
                </li>
            </ul>
            <p><a class="anchor" id="todo"></a> </p>
            <h2>Further improvements</h2>
            <p>In the following we outline the main problems currently known and possible remedies. You are very welcome
                to contribute to this pipeline to make it even better!</p>
            <table class="doxtable">
                <tr>
                    <th align="left">What can go wrong</th>
                    <th align="left">TODO </th>
                </tr>
                <tr>
                    <td align="left">Randomness due to OS scheduler, unreliable rosbags</td>
                    <td align="left">Implement rosbag data provider pattern, and ensure correctness of events
                        consumption </td>
                </tr>
                <tr>
                    <td align="left"></td>
                    <td align="left">Currently the pipeline uses multiple nodes. Switching to nodelets or using a single
                        node could improve the repeatability of the rosbags. </td>
                </tr>
                <tr>
                    <td align="left">Robustness</td>
                    <td align="left">bootstrapping: we should catch whenever SVO does not converge and trigger an
                        automatic restart (what a human operator would eventually do manually). </td>
                </tr>
                <tr>
                    <td align="left"></td>
                    <td align="left">tracking: catch whenever the tracker diverges, and re-initialize. Currently we have
                        to parameters to predict this situation, namely <code>min_map_size</code> and
                        <code>min_n_keypoints</code>.
                    </td>
                </tr>
                <tr>
                    <td align="left">Unreliable bootstrapping</td>
                    <td align="left">Currently we have two working ways to bootstrap the pipeline from events: from SVO
                        (feeding it with events frames) and with a fronto-planar assumption. </td>
                </tr>
                <tr>
                    <td align="left"></td>
                    <td align="left">Reducing the assumptions required and making them more reliable would allow a
                        better bootstrapping, reducing the gap to the bootstrapping from traditional frames
                        (<code>bootstrap_image_topic:=/dvs/image_raw</code>). </td>
                </tr>
            </table>
            <p><a class="anchor" id="ref"></a> </p>
            <h2>Additional resources on Event Cameras</h2>
            <ul>
                <li><a href="http://rpg.ifi.uzh.ch/docs/EventVisionSurvey.pdf">Event-based Vision Survey</a></li>
                <li><a href="https://github.com/uzh-rpg/event-based_vision_resources">List of Event-based Vision
                        Resources</a></li>
                <li><a href="http://rpg.ifi.uzh.ch/davis_data.html">Event Camera Dataset</a></li>
                <li><a href="http://rpg.ifi.uzh.ch/esim">Event Camera Simulator</a></li>
                <li><a href="http://rpg.ifi.uzh.ch/research_dvs.html">RPG research page on Event Cameras</a></li>
                <li><a href="http://rpg.ifi.uzh.ch/research_calib.html">RPG research page on sensor calibration</a>
                </li>
            </ul>
        </div>
    </div><!-- contents -->
    <!-- start footer part -->
    <hr class="footer" />
    <address class="footer"><small>
            Generated by &#160;<a href="http://www.doxygen.org/index.html">
                <img class="footer" src="doxygen.png" alt="doxygen" />
            </a> 1.8.13
        </small></address>
</body>

</html>